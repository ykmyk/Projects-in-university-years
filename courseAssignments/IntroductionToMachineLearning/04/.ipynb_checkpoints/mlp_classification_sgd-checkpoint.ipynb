{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74fa332d-93d9-4af8-82b4-822181a50b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# These arguments will be set appropriately by ReCodEx, even if you change them.\n",
    "parser.add_argument(\"--batch_size\", default=10, type=int, help=\"Batch size\")\n",
    "parser.add_argument(\"--classes\", default=10, type=int, help=\"Number of classes to use\")\n",
    "parser.add_argument(\"--epochs\", default=10, type=int, help=\"Number of SGD training epochs\")\n",
    "parser.add_argument(\"--hidden_layer\", default=50, type=int, help=\"Hidden layer size\")\n",
    "parser.add_argument(\"--learning_rate\", default=0.01, type=float, help=\"Learning rate\")\n",
    "parser.add_argument(\"--recodex\", default=False, action=\"store_true\", help=\"Running in ReCodEx\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed\")\n",
    "parser.add_argument(\"--test_size\", default=797, type=lambda x: int(x) if x.isdigit() else float(x), help=\"Test size\")\n",
    "# If you add more arguments, ReCodEx will keep them with your default values.\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0514d52f-6342-4cef-b8fd-ca83ec29b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random generator with a given seed.\n",
    "generator = np.random.RandomState(args.seed)\n",
    "\n",
    "# Load the digits dataset.\n",
    "data, target = sklearn.datasets.load_digits(n_class=args.classes, return_X_y=True)\n",
    "\n",
    "# Split the dataset into a train set and a test set.\n",
    "# Use `sklearn.model_selection.train_test_split` method call, passing\n",
    "# arguments `test_size=args.test_size, random_state=args.seed`.\n",
    "train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(\n",
    "    data, target, test_size=args.test_size, random_state=args.seed)\n",
    "\n",
    "# Generate initial model weights.\n",
    "weights = [generator.uniform(size=[train_data.shape[1], args.hidden_layer], low=-0.1, high=0.1),\n",
    "           generator.uniform(size=[args.hidden_layer, args.classes], low=-0.1, high=0.1)]\n",
    "biases = [np.zeros(args.hidden_layer), np.zeros(args.classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "017b0cf0-db79-4438-9b0a-38d51b81ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(x, 0.0)\n",
    "\n",
    "def ReLU_g(x):\n",
    "    return (x > 0).astype(x.dtype)\n",
    "\n",
    "def softMax(x):\n",
    "    exp_ = np.exp(x - np.max(x))\n",
    "    return exp_ / np.sum(exp_)\n",
    "\n",
    "def cross_entropy(probs, t):\n",
    "    return -np.mean(np.log(probs[np.arange(probs.shape[0]), t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dd94134-772f-4822-8303-5b1baa8a2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs):\n",
    "    # TODO: Implement forward propagation, returning *both* the value of the hidden\n",
    "    # layer and the value of the output layer.\n",
    "    #\n",
    "    # We assume a neural network with a single hidden layer of size `args.hidden_layer`\n",
    "    # and ReLU activation, where $ReLU(x) = max(x, 0)$, and an output layer with softmax\n",
    "    # activation.\n",
    "        \n",
    "    # The value of the hidden layer is computed as `ReLU(inputs @ weights[0] + biases[0])`.\n",
    "    h_in = inputs @ weights[0] + biases[0]\n",
    "    h_out = ReLU(h_in)\n",
    "    # The value of the output layer is computed as `softmax(hidden_layer @ weights[1] + biases[1])`.\n",
    "    o_in = h_out @ weights[1] + biases[1]\n",
    "    output = softMax(o_in)\n",
    "    \n",
    "    # Note that you need to be careful when computing softmax, because the exponentiation\n",
    "    # in softmax can easily overflow. To avoid it, you should use the fact that\n",
    "    # $softmax(z) = softmax(z + any_constant)$ and compute $softmax(z) = softmax(z - maximum_of_z)$.\n",
    "    # That way we only exponentiate values which are non-positive, and overflow does not occur.\n",
    "    # raise NotImplementedError()\n",
    "\n",
    "    return h_in, h_out, o_in, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99d44d88-ec30-4f51-8a4e-4d26e14531a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m     biases[\u001b[32m0\u001b[39m] -= args.learning_rate * gb0;\n\u001b[32m     44\u001b[39m     biases[\u001b[32m1\u001b[39m] -= args.learning_rate * gb1;\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m pred_train = \u001b[43msigmoid\u001b[49m(X_batch @ weights)\n\u001b[32m     47\u001b[39m pred_test = sigmoid(test_data @ weights)\n\u001b[32m     49\u001b[39m t_train = np.zeros(\u001b[38;5;28mlen\u001b[39m(pred_train), dtype=\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sigmoid' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    permutation = generator.permutation(train_data.shape[0])\n",
    "\n",
    "    # TODO: Process the data in the order of `permutation`. For every\n",
    "    # `args.batch_size` of them, average their gradient, and update the weights.\n",
    "    # You can assume that `args.batch_size` exactly divides `train_data.shape[0]`.\n",
    "    b_size = args.batch_size\n",
    "    for i in range(0, train_data.shape[0], b_size):\n",
    "        indices= permutation[i : i + b_size]\n",
    "        X_batch = train_data[indices]\n",
    "        t_batch = train_target[indices]\n",
    "        h_in, h_out, o_in, output = forward(X_batch)\n",
    "        \n",
    "        # The gradient used in SGD has now four parts, gradient of `weights[0]` and `weights[1]`\n",
    "        # and gradient of `biases[0]` and `biases[1]`.\n",
    "        #\n",
    "        # You can either compute the gradient directly from the neural network formula,\n",
    "        # i.e., as a gradient of $-log P(target | data)$, or you can compute\n",
    "        # it step by step using the chain rule of derivatives, in the following order:\n",
    "        # - compute the derivative of the loss with respect to *inputs* of the\n",
    "        #   softmax on the last layer,\n",
    "        b = t_batch.shape[0]\n",
    "        g = output.copy()\n",
    "        g[np.arange(b), t_batch] -= 1\n",
    "        g = g / b\n",
    "        \n",
    "        # - compute the derivative with respect to `weights[1]` and `biases[1]`,\n",
    "        gw1 = h_out.T @ g\n",
    "        gb1 = np.sum(g, axis=0)\n",
    "        \n",
    "        # - compute the derivative with respect to the hidden layer output,\n",
    "        deriv_out = g @ weights[1].T\n",
    "            \n",
    "        # - compute the derivative with respect to the hidden layer input,\n",
    "        deriv_in = deriv_out * ReLU_g(h_in)\n",
    "            \n",
    "        # - compute the derivative with respect to `weights[0]` and `biases[0]`.\n",
    "        gw0 = X_batch.T @ deriv_in\n",
    "        gb0 = np.sum(deriv_in, axis=0)\n",
    "\n",
    "        weights[0] -= args.learning_rate * gw0;\n",
    "        weights[1] -= args.learning_rate * gw1;\n",
    "        biases[0] -= args.learning_rate * gb0;\n",
    "        biases[1] -= args.learning_rate * gb1;\n",
    "        \n",
    "    _, _, _, train_prob = forward(train_data)\n",
    "    train_pred = np.argmax(train_prob, axis=1)\n",
    "    train_acc = np.mean(train_pred == train_target)\n",
    "    train_loss = cross_entropy()\n",
    "\n",
    "    # log(pred_train)^t_train = t_train * log(pred_train)\n",
    "    train_loss = np.mean(- t_train * np.log(pred_train) - (1 - t_train) * np.log(1 - pred_train))\n",
    "    test_loss = np.mean(- t_test * np.log(pred_test) - (1 - t_test) * np.log(1 - pred_test))\n",
    "    \n",
    "    train_accuracy /= b_size\n",
    "    test_accuracy /= b_size\n",
    "\n",
    "    print(\"After epoch {}: train acc {:.1f}%, test acc {:.1f}%\".format(\n",
    "        epoch + 1, 100 * train_accuracy, 100 * test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b703ab9-eb2e-4bb4-8bbb-147bba36d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuple(weights + biases), [100 * train_accuracy, 100 * test_accuracy])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
