{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62dd9915-6019-4904-ae77-db794defc448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "import lzma\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from typing import Optional\n",
    "import urllib.request\n",
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# These arguments will be set appropriately by ReCodEx, even if you change them.\n",
    "parser.add_argument(\"--predict\", default=None, type=str, help=\"Path to the dataset to predict\")\n",
    "parser.add_argument(\"--recodex\", default=False, action=\"store_true\", help=\"Running in ReCodEx\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed\")\n",
    "# For these and any other arguments you add, ReCodEx will keep your default value.\n",
    "parser.add_argument(\"--model_path\", default=\"mnist_competition.model\", type=str, help=\"Model path\")\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b761c295-a978-4a20-8e0b-12f9f3041e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"MNIST Dataset.\n",
    "\n",
    "    The train set contains 60000 images of handwritten digits. The data\n",
    "    contain 28*28=784 values in the range 0-255, the targets are numbers 0-9.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 name=\"mnist.train.npz\",\n",
    "                 data_size=None,\n",
    "                 url=\"https://ufal.mff.cuni.cz/~courses/npfl129/2526/datasets/\"):\n",
    "        if not os.path.exists(name):\n",
    "            print(\"Downloading dataset {}...\".format(name), file=sys.stderr)\n",
    "            urllib.request.urlretrieve(url + name, filename=\"{}.tmp\".format(name))\n",
    "            os.rename(\"{}.tmp\".format(name), name)\n",
    "\n",
    "        # Load the dataset, i.e., `data` and optionally `target`.\n",
    "        dataset = np.load(name)\n",
    "        for key, value in dataset.items():\n",
    "            setattr(self, key, value[:data_size])\n",
    "        self.data = self.data.reshape([-1, 28*28]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e809036-37ad-4f66-b14f-52c94d9924af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLPClassifier' object has no attribute 'coefs_'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# If you trained one or more MLPs, you can use the following code\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# to compress it significantly (approximately 12 times). The snippet\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# assumes the trained `MLPClassifier` is in the `mlp` variable.\u001b[39;00m\n\u001b[32m     21\u001b[39m mlp._optimizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(mlp.coefs_)): mlp.coefs_[i] = mlp.coefs_[i].astype(np.float16)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(mlp.intercepts_)): mlp.intercepts_[i] = mlp.intercepts_[i].astype(np.float16)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Serialize the model.\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'MLPClassifier' object has no attribute 'coefs_'"
     ]
    }
   ],
   "source": [
    "if args.predict is None:\n",
    "    # We are training a model.\n",
    "    np.random.seed(args.seed)\n",
    "    train = Dataset()\n",
    "    X = train.data\n",
    "    y = train.target\n",
    "    # train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(\n",
    "    #                                         X, y, test_size=100, random_state=args.seed)\n",
    "\n",
    "    # activation_ = {'identity', 'logistic', 'relu', 'tanh'}\n",
    "    # solver_ = {'lbfgs', 'sgd', 'adam'}\n",
    "    # TODO: Train a model on the given dataset and store it in `model`.\n",
    "    model = sklearn.neural_network.MLPClassifier(activation='logistic', solver='adam')\n",
    "    model.fit(X, y)\n",
    "    # print(model.score(test_data, test_target))\n",
    "    \n",
    "    \n",
    "    # If you trained one or more MLPs, you can use the following code\n",
    "    # to compress it significantly (approximately 12 times). The snippet\n",
    "    # assumes the trained `MLPClassifier` is in the `mlp` variable.\n",
    "    mlp._optimizer = None\n",
    "    for i in range(len(mlp.coefs_)): mlp.coefs_[i] = mlp.coefs_[i].astype(np.float16)\n",
    "    for i in range(len(mlp.intercepts_)): mlp.intercepts_[i] = mlp.intercepts_[i].astype(np.float16)\n",
    "\n",
    "    # Serialize the model.\n",
    "    with lzma.open(args.model_path, \"wb\") as model_file:\n",
    "        pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53541b-fe7d-4038-8a7d-4ea568440996",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.predict is not None:\n",
    "    # Use the model and return test set predictions, either as a Python list or a NumPy array.\n",
    "    test = Dataset(args.predict)\n",
    "\n",
    "    with lzma.open(args.model_path, \"rb\") as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "\n",
    "    # TODO: Generate `predictions` with the test set predictions.\n",
    "    predictions = ...\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17034f-eb8e-484f-9d1f-837e3390ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_args = parser.parse_args([] if \"__file__\" not in globals() else None)\n",
    "    main(main_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa0bf4-cbbf-42d6-8cca-7387b08848fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "train = Dataset()\n",
    "X = train.data\n",
    "y = train.target\n",
    "train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(\n",
    "                                        X, y, test_size=100, random_state=args.seed)\n",
    "\n",
    "# activation_ = {'identity', 'logistic', 'relu', 'tanh'}\n",
    "# solver_ = {'lbfgs', 'sgd', 'adam'}\n",
    "# TODO: Train a model on the given dataset and store it in `model`.\n",
    "model = sklearn.neural_network.MLPClassifier(activation='identity', solver='sgd')\n",
    "model.fit(train_data, train_target)\n",
    "print(model.score(test_data, test_target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
