{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0cd5dde-0e02-4c79-8476-3241d2ef5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# These arguments will be set appropriately by ReCodEx, even if you change them.\n",
    "parser.add_argument(\"--batch_size\", default=10, type=int, help=\"Batch size\")\n",
    "parser.add_argument(\"--classes\", default=10, type=int, help=\"Number of classes to use\")\n",
    "parser.add_argument(\"--epochs\", default=10, type=int, help=\"Number of SGD training epochs\")\n",
    "parser.add_argument(\"--hidden_layer\", default=50, type=int, help=\"Hidden layer size\")\n",
    "parser.add_argument(\"--learning_rate\", default=0.01, type=float, help=\"Learning rate\")\n",
    "parser.add_argument(\"--recodex\", default=False, action=\"store_true\", help=\"Running in ReCodEx\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed\")\n",
    "parser.add_argument(\"--test_size\", default=797, type=lambda x: int(x) if x.isdigit() else float(x), help=\"Test size\")\n",
    "# If you add more arguments, ReCodEx will keep them with your default values.\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed01fa44-ad10-49c7-9a0c-5cc4450b0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = np.random.RandomState(args.seed)\n",
    "\n",
    "# Load the digits dataset.\n",
    "data, target = sklearn.datasets.load_digits(n_class=args.classes, return_X_y=True)\n",
    "\n",
    "# Append a constant feature with value 1 to the end of all input data.\n",
    "# Then we do not need to explicitly represent bias - it becomes the last weight.\n",
    "data = np.pad(data, [(0, 0), (0, 1)], constant_values=1)\n",
    "\n",
    "# Split the dataset into a train set and a test set.\n",
    "# Use `sklearn.model_selection.train_test_split` method call, passing\n",
    "# arguments `test_size=args.test_size, random_state=args.seed`.\n",
    "train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(\n",
    "    data, target, test_size=args.test_size, random_state=args.seed)\n",
    "\n",
    "# Generate initial model weights.\n",
    "weights = generator.uniform(size=[train_data.shape[1], args.classes], low=-0.1, high=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841db513-e8bf-402f-ad8e-54e0549913f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMax(x):\n",
    "    if x.ndim == 1:\n",
    "        exp_ = np.exp(x - np.max(x))\n",
    "        return exp_ / np.sum(exp_)\n",
    "    else:\n",
    "        exp_ = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_ / np.sum(exp_, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2c5a6c1-2e4b-4097-939e-ce33f026fdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 1: train loss 0.2536 acc 91.7%, test loss 0.3084 acc 90.6%\n",
      "After epoch 2: train loss 0.1520 acc 94.9%, test loss 0.2402 acc 92.2%\n",
      "After epoch 3: train loss 0.0845 acc 97.3%, test loss 0.1391 acc 96.0%\n",
      "After epoch 4: train loss 0.1290 acc 96.0%, test loss 0.1716 acc 94.5%\n",
      "After epoch 5: train loss 0.0810 acc 97.4%, test loss 0.1619 acc 94.9%\n",
      "After epoch 6: train loss 0.0634 acc 98.3%, test loss 0.1259 acc 96.1%\n",
      "After epoch 7: train loss 0.0621 acc 98.3%, test loss 0.1243 acc 96.5%\n",
      "After epoch 8: train loss 0.0441 acc 99.0%, test loss 0.1142 acc 97.0%\n",
      "After epoch 9: train loss 0.0651 acc 97.9%, test loss 0.1302 acc 96.0%\n",
      "After epoch 10: train loss 0.0425 acc 98.8%, test loss 0.1157 acc 96.5%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "        permutation = generator.permutation(train_data.shape[0])\n",
    "        # TODO: Process the data in the order of `permutation`. For every\n",
    "        # `args.batch_size` of them, average their gradient, and update the weights.\n",
    "        # You can assume that `args.batch_size` exactly divides `train_data.shape[0]`.\n",
    "        b_size = args.batch_size\n",
    "        for i in range(0, train_data.shape[0], b_size):\n",
    "            indices= permutation[i : i + b_size]\n",
    "            X_batch = train_data[indices]\n",
    "            t_batch = train_target[indices]\n",
    "            probs= softMax(X_batch @ weights)\n",
    "\n",
    "            g = probs.copy()\n",
    "            g[np.arange(b_size), t_batch] -= 1\n",
    "            g = X_batch.T @ g / b_size\n",
    "\n",
    "            weights -= args.learning_rate * g\n",
    "        # Note that you need to be careful when computing softmax because the exponentiation\n",
    "        # in softmax can easily overflow. To avoid it, you should use the fact that\n",
    "        # $softmax(z) = softmax(z + any_constant)$ and compute $softmax(z) = softmax(z - maximum_of_z)$.\n",
    "        # That way we only exponentiate non-positive values, and overflow does not occur.\n",
    "\n",
    "        # TODO: After the SGD epoch, measure the average loss and accuracy for both the\n",
    "        # train test and the test set. The loss is the average MLE loss (i.e., the\n",
    "        # negative log-likelihood, or cross-entropy loss, or KL loss) per example.\n",
    "        train_probs = softMax(train_data @ weights)\n",
    "        train_pred = np.argmax(train_probs, axis=1)\n",
    "        train_accuracy = np.mean(train_pred == train_target)\n",
    "        train_loss = -np.mean(np.log(train_probs[np.arange(len(train_target)), train_target]))\n",
    "\n",
    "        test_probs = softMax(test_data @ weights)\n",
    "        test_pred = np.argmax(test_probs, axis=1)\n",
    "        test_accuracy = np.mean(test_pred == test_target)\n",
    "        test_loss = -np.mean(np.log(test_probs[np.arange(len(test_target)), test_target]))\n",
    "\n",
    "        print(\"After epoch {}: train loss {:.4f} acc {:.1f}%, test loss {:.4f} acc {:.1f}%\".format(\n",
    "            epoch + 1, train_loss, 100 * train_accuracy, test_loss, 100 * test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
