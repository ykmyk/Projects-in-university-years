{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a44ee4a1-098a-4fa1-b1f3-80cf8e32b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# These arguments will be set appropriately by ReCodEx, even if you change them.\n",
    "parser.add_argument(\"--batch_size\", default=10, type=int, help=\"Batch size\")\n",
    "parser.add_argument(\"--classes\", default=5, type=int, help=\"Number of classes to use\")\n",
    "parser.add_argument(\"--data_size\", default=200, type=int, help=\"Data size\")\n",
    "parser.add_argument(\"--epochs\", default=2, type=int, help=\"Number of SGD training epochs\")\n",
    "parser.add_argument(\"--learning_rate\", default=0.01, type=float, help=\"Learning rate\")\n",
    "parser.add_argument(\"--recodex\", default=False, action=\"store_true\", help=\"Running in ReCodEx\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed\")\n",
    "parser.add_argument(\"--test_size\", default=0.5, type=lambda x: int(x) if x.isdigit() else float(x), help=\"Test size\")\n",
    "# If you add more arguments, ReCodEx will keep them with your default values.\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "db00ce04-4856-47a5-bb79-eb8c3cc61707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random generator with a given seed.\n",
    "generator = np.random.RandomState(args.seed)\n",
    "\n",
    "# Generate an artificial classification dataset.\n",
    "data, target_list = sklearn.datasets.make_multilabel_classification(\n",
    "    n_samples=args.data_size, n_classes=args.classes, allow_unlabeled=False,\n",
    "    return_indicator=False, random_state=args.seed)\n",
    "\n",
    "# TODO: The `target` is a list of classes for every input example. Convert\n",
    "# it to a dense representation (n-hot encoding) -- for each input example,\n",
    "# the target should be vector of `args.classes` binary indicators.\n",
    "\n",
    "# create the list of zeros with row size = len(target_list0 and column size = args.classes\n",
    "target = np.zeros((len(target_list), args.classes), dtype=np.float32)\n",
    "\n",
    "# assign 1 with the index contained in \"labels\"\n",
    "# ex.) target[0, [0,2]] = 1.0\n",
    "# then produce [1, 0, 1, 0]\n",
    "for i, labels in enumerate(target_list):\n",
    "    target[i, labels] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f6234b1f-2614-4aaa-836a-36920ac36657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a constant feature with value 1 to the end of all input data.\n",
    "# Then we do not need to explicitly represent bias - it becomes the last weight.\n",
    "data = np.pad(data, [(0, 0), (0, 1)], constant_values=1)\n",
    "\n",
    "# Split the dataset into a train set and a test set.\n",
    "# Use `sklearn.model_selection.train_test_split` method call, passing\n",
    "# arguments `test_size=args.test_size, random_state=args.seed`.\n",
    "train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(\n",
    "    data, target, test_size=args.test_size, random_state=args.seed)\n",
    "\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "train_target = torch.tensor(train_target, dtype=torch.float32)\n",
    "test_target = torch.tensor(test_target, dtype=torch.float32)\n",
    "\n",
    "model = MultilayerPerceptron(train_data.shape[1], 100 , args.classes)  # We do not explicitly specify the weigh initialization interval\n",
    "loss_f = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# Generate initial model weights.\n",
    "weights = generator.uniform(size=[train_data.shape[1], args.classes], low=-0.1, high=0.1)\n",
    "\n",
    "TP = torch.zeros(args.classes, dtype=torch.long)\n",
    "TN = torch.zeros(args.classes, dtype=torch.long)\n",
    "FP = torch.zeros(args.classes, dtype=torch.long)\n",
    "FN = torch.zeros(args.classes, dtype=torch.long)\n",
    "\n",
    "test_TP = torch.zeros(args.classes, dtype=torch.long)\n",
    "test_TN = torch.zeros(args.classes, dtype=torch.long)\n",
    "test_FP = torch.zeros(args.classes, dtype=torch.long)\n",
    "test_FN = torch.zeros(args.classes, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "91452148-8ba2-42e9-b05c-070708db0b43",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor of type LongTensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m batch_train = train_data[indices]\n\u001b[32m     11\u001b[39m batch_target = train_target[indices]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m batch_target_probs = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m)\u001b[49m.type(torch.float32)\n\u001b[32m     14\u001b[39m optimizer.zero_grad()\n\u001b[32m     15\u001b[39m pred = model(batch_train)\n",
      "\u001b[31mRuntimeError\u001b[39m: one_hot is only applicable to index tensor of type LongTensor."
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    permutation = generator.permutation(train_data.shape[0])\n",
    "\n",
    "    # TODO: Process the data in the order of `permutation`. For every\n",
    "    # `args.batch_size` of them, average their gradient, and update the weights.\n",
    "    # You can assume that `args.batch_size` exactly divides `train_data.shape[0]`.\n",
    "    b_size = args.batch_size\n",
    "    for i in range(0, train_data.shape[0], b_size):\n",
    "        indices= permutation[i : i + b_size]\n",
    "        batch_train = train_data[indices]\n",
    "        batch_target = train_target[indices]\n",
    "        # batch_target_probs = torch.nn.functional.one_hot(batch_target, num_classes=args.classes).type(torch.float32)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch_train)\n",
    "        loss = loss_f(pred, batch_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# # TODO: After the SGD epoch, compute the micro-averaged and the\n",
    "# # macro-averaged F1-score for both the train test and the test set.\n",
    "# # Compute these scores manually, without using `sklearn.metrics`.\n",
    "# train_f1_micro, train_f1_macro, test_f1_micro, test_f1_macro = ...\n",
    "    train_pred = (torch.sigmoid(model(train_data))>= 0.5)\n",
    "    test_pred = (torch.sigmoid(model(test_data))>= 0.5)\n",
    "\n",
    "    train_f1_micro, train_f1_macro = f1_micro_macro(y_train, train_target)\n",
    "    test_f1_micro, test_f1_macro = f1_micro_macro(y_test, test_target)\n",
    "\n",
    "\n",
    "# print(\"After epoch {}: trai F1 micro {:.2f}% macro {:.2f}%, test F1 micro {:.2f}% macro {:.1f}%\".format(\n",
    "#     epoch + 1, 100 * train_f1_micro, 100 * train_f1_macro, 100 * test_f1_micro, 100 * test_f1_macro))\n",
    "\n",
    "print(train_f1_micro.item())\n",
    "print(train_f1_macro.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f4118dd4-68d0-4c51-8b1d-54bdb4c47d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # No sigmoid here â€” BCEWithLogitsLoss applies it internally.\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b73d06e8-e4d8-4d39-9ac0-a3a86a8a4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_micro_macro(y_true, y_pred):\n",
    "    y_true = y_true.bool()\n",
    "    y_pred = y_pred.bool()\n",
    "    \n",
    "    tp = (pred_binary &  true).sum(dim=0)\n",
    "    fp = (pred_binary & ~true).sum(dim=0)\n",
    "    fn = (~pred_binary &  true).sum(dim=0)\n",
    "    tn = (~pred_binary & ~true).sum(dim=0)\n",
    "    \n",
    "    TPmic = TP.sum()\n",
    "    TNmic = TN.sum()\n",
    "    FNmic = FN.sum()\n",
    "    FPmic = FP.sum()\n",
    "\n",
    "    f1_micro = (2*TPmic) / (TPmic*2 + FNmic + FPmic)\n",
    "    f1_macro = ((2*TP) / (TP*2 + FN + FP)).mean()\n",
    "\n",
    "    return f1_micro, f1_macro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
