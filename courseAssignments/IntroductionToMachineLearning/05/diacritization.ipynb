{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea002abe-bb3d-4336-bf8d-4365cae25c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "import lzma\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from typing import Optional\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# These arguments will be set appropriately by ReCodEx, even if you change them.\n",
    "parser.add_argument(\"--predict\", default=None, type=str, help=\"Path to the dataset to predict\")\n",
    "parser.add_argument(\"--recodex\", default=False, action=\"store_true\", help=\"Running in ReCodEx\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed\")\n",
    "# For these and any other arguments you add, ReCodEx will keep your default value.\n",
    "parser.add_argument(\"--model_path\", default=\"diacritization.model\", type=str, help=\"Model path\")\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e85badf9-d78c-4bbe-b237-e71ceede27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    LETTERS_NODIA = \"acdeeinorstuuyz\"\n",
    "    LETTERS_DIA = \"áčďéěíňóřšťúůýž\"\n",
    "\n",
    "    # A translation table usable with `str.translate` to rewrite characters with dia to the ones without them.\n",
    "    DIA_TO_NODIA = str.maketrans(LETTERS_DIA + LETTERS_DIA.upper(), LETTERS_NODIA + LETTERS_NODIA.upper())\n",
    "\n",
    "    def __init__(self,\n",
    "                 name=\"fiction-train.txt\",\n",
    "                 url=\"https://ufal.mff.cuni.cz/~courses/npfl129/2526/datasets/\"):\n",
    "        if not os.path.exists(name):\n",
    "            print(\"Downloading dataset {}...\".format(name), file=sys.stderr)\n",
    "            licence_name = name.replace(\".txt\", \".LICENSE\")\n",
    "            urllib.request.urlretrieve(url + licence_name, filename=licence_name)\n",
    "            urllib.request.urlretrieve(url + name, filename=\"{}.tmp\".format(name))\n",
    "            os.rename(\"{}.tmp\".format(name), name)\n",
    "\n",
    "        # Load the dataset and split it into `data` and `target`.\n",
    "        with open(name, \"r\", encoding=\"utf-8-sig\") as dataset_file:\n",
    "            self.target = dataset_file.read()\n",
    "        self.data = self.target.translate(self.DIA_TO_NODIA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e26b1d8-525a-4822-9808-4bca29d78bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are training a model.\n",
    "np.random.seed(args.seed)\n",
    "train = Dataset()\n",
    "X = []\n",
    "y = []\n",
    "window = 2\n",
    "\n",
    "for i in range(window, len(train.data) - window):\n",
    "    if train.data[i].lower() in Dataset.LETTERS_NODIA:\n",
    "        context = train.data[i - window : i + window + 1]\n",
    "        X.append(context)\n",
    "        y.append(train_target[i])\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(1, 5))\n",
    "X = vectorizer.fit_transform(X)\n",
    "y = np.array(y)\n",
    "\n",
    "model = sklearn.LogisticRegression(\n",
    "    multi_class=\"multinomial\", solver=\"saga\",\n",
    "    C=1.0, max_iter=500, n_jobs=-1\n",
    ")\n",
    "model.fit(X, y)\n",
    "\n",
    "predictions = \"\"\n",
    "for i in range(len(test.data)):\n",
    "    if test.data[i].lower() in Dataset.LETTERS_NODIA:\n",
    "        # Predict correct diacritic using context window and model\n",
    "        context = test.data[i - window : i + window + 1]\n",
    "        X = vectorizer.transform([context])\n",
    "        pred_letter = model.predict(X)[0]\n",
    "        predictions += pred_letter\n",
    "    else:\n",
    "        # Keep spaces and punctuation unchanged\n",
    "        predictions += test.data[i]\n",
    "# Serialize the model.\n",
    "with lzma.open(args.model_path, \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f510d9-f80b-418f-ad71-88ccac085bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(gold: str, system: str) -> float:\n",
    "    assert isinstance(gold, str) and isinstance(system, str), \"The gold and system outputs must be strings\"\n",
    "\n",
    "    gold, system = gold.split(), system.split()\n",
    "    assert len(gold) == len(system), \\\n",
    "        \"The test data has {} words, but got {} instead, aborting\".format(len(gold), len(system))\n",
    "\n",
    "    words, correct = 0, 0\n",
    "    for gold_token, system_token in zip(gold, system):\n",
    "        words += 1\n",
    "        correct += gold_token == system_token\n",
    "\n",
    "    return correct / words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
