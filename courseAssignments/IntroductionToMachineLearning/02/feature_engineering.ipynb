{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bc4ed0-5c2c-4a2d-b70b-8243376e84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.compose\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", default=\"diabetes\", type=str)\n",
    "parser.add_argument(\"--recodex\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int)\n",
    "parser.add_argument(\"--test_size\", default=0.5, type=lambda x: int(x) if x.isdigit() else float(x))\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f83b56f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mType:\u001b[39m        tuple\n",
       "\u001b[31mString form:\u001b[39m (442, 11)\n",
       "\u001b[31mLength:\u001b[39m      2\n",
       "\u001b[31mDocstring:\u001b[39m  \n",
       "Built-in immutable sequence.\n",
       "\n",
       "If no argument is given, the constructor returns an empty tuple.\n",
       "If iterable is specified the tuple is initialized from iterable's items.\n",
       "\n",
       "If the argument is a tuple, the return value is the same object."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = getattr(sklearn.datasets, \"load_{}\".format(args.dataset))()\n",
    "d = dataset.data\n",
    "y = dataset.target\n",
    "X = np.c_[d, np.ones(d.shape[0], dtype=int)]\n",
    "\n",
    "X.shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e70aba8-6c68-4c06-b641-b0d704a13053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `ndarray` not found.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Split the dataset into a train set and a test set.\n",
    "# Use `sklearn.model_selection.train_test_split` method call, passing\n",
    "# arguments `test_size=args.test_size, random_state=args.seed`.\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "                                        X, y, test_size=args.test_size, random_state=args.seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "14fcb804-3c98-496c-bd52-7aa4b2444ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_integer_only(col : np.ndarray) -> bool:\n",
    "    return np.all(col == np.round(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7591b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - if a column has only integer values, consider it a categorical column\n",
    "#   (days in a week, dog breed, ...; in general, integer values can also\n",
    "#   represent numerical non-categorical values, but we use this assumption\n",
    "#   for the sake of exercise). Encode the values with one-hot encoding\n",
    "#   using `sklearn.preprocessing.OneHotEncoder` (note that its output is by\n",
    "#   default sparse, you can use `sparse_output=False` to generate dense output;\n",
    "#   also use `handle_unknown=\"ignore\"` to ignore missing values in test set).\n",
    "d_size = X_train.shape[1]\n",
    "int_ind = [i for i in range(d_size) if is_integer_only(X_train[:, i])]\n",
    "rest_ind = [i for i in range(d_size) if i not in int_ind]\n",
    "\n",
    "# - for the rest of the columns, normalize their values so that they\n",
    "#   have mean 0 and variance 1; use `sklearn.preprocessing.StandardScaler`.        \n",
    "\n",
    "preprocess = sklearn.compose.ColumnTransformer(\n",
    "        [(\"ints\", sklearn.preprocessing.OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), int_ind),\n",
    "        (\"rests\", sklearn.preprocessing.StandardScaler(), rest_ind)]\n",
    ")\n",
    "\n",
    "X_train_trans = preprocess.fit_transform(X_train)\n",
    "X_test_trans = preprocess.transform(X_test)\n",
    "\n",
    "# In the output, first there should be all the one-hot categorical features,\n",
    "# and then the real-valued features. To process different dataset columns\n",
    "# differently, you can use `sklearn.compose.ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8973f-3061-4c0a-afa0-167d6ff172b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "60e62342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: To the current features, append polynomial features of order 2.\n",
    "# If the input values are `[a, b, c, d]`, you should append\n",
    "# `[a^2, ab, ac, ad, b^2, bc, bd, c^2, cd, d^2]`. You can generate such polynomial\n",
    "# features either manually, or you can employ the provided transformer\n",
    "#   sklearn.preprocessing.PolynomialFeatures(2, include_bias=False)\n",
    "# which appends such polynomial features of order 2 to the given features.\n",
    "\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_trans)\n",
    "X_test_poly = poly.transform(X_test_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3ea06d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: You can wrap all the feature processing steps into one transformer\n",
    "# by using `sklearn.pipeline.Pipee`. Although not strictly needed, it is\n",
    "# usually comfortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "97523f6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (1278992443.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn train_data[:5], test_data[:5]\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# TODO: Fit the feature preprocessing steps (the composed pipeline with all of\n",
    "# them; or the individual steps, if you prefer) on the training data (using `fit`).\n",
    "# Then transform the training data into `train_data` (with a `transform` call;\n",
    "# however, you can combine the two methods into a single `fit_transform` call).\n",
    "# Finally, transform testing data to `test_data`.\n",
    "train_data = X_train_poly\n",
    "test_data = X_test_poly\n",
    "return train_data[:5], test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f95ad-cfb1-49bf-9017-0b0257ff9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_args = parser.parse_args([] if \"__file__\" not in globals() else None)\n",
    "    train_data, test_data = main(main_args)\n",
    "    for dataset in [train_data, test_data]:\n",
    "        for line in range(min(dataset.shape[0], 5)):\n",
    "            print(\" \".join(\"{:.4g}\".format(dataset[line, column]) for column in range(min(dataset.shape[1], 140))),\n",
    "                  *[\"...\"] if dataset.shape[1] > 140 else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321cfbe-72b8-4aeb-847c-bf74111d3ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f0acd-bcd9-4d8b-8d9f-17a4c2198060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe18626-c9ef-45ca-8b9a-a963f8b07af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
