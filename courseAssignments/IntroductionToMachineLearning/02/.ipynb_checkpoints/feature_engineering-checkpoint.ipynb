{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bc4ed0-5c2c-4a2d-b70b-8243376e84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.compose\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset\", default=\"diabetes\", type=str)\n",
    "parser.add_argument(\"--recodex\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--seed\", default=42, type=int)\n",
    "parser.add_argument(\"--test_size\", default=0.5, type=lambda x: int(x) if x.isdigit() else float(x))\n",
    "args = parser.parse_args([] if \"__file__\" not in globals() else None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f83b56f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mType:\u001b[39m        tuple\n",
       "\u001b[31mString form:\u001b[39m (442, 11)\n",
       "\u001b[31mLength:\u001b[39m      2\n",
       "\u001b[31mDocstring:\u001b[39m  \n",
       "Built-in immutable sequence.\n",
       "\n",
       "If no argument is given, the constructor returns an empty tuple.\n",
       "If iterable is specified the tuple is initialized from iterable's items.\n",
       "\n",
       "If the argument is a tuple, the return value is the same object."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = getattr(sklearn.datasets, \"load_{}\".format(args.dataset))()\n",
    "d = dataset.data\n",
    "y = dataset.target\n",
    "X = np.c_[d, np.ones(d.shape[0], dtype=int)]\n",
    "\n",
    "X.shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "218c3208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `ndarray` not found.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Split the dataset into a train set and a test set.\n",
    "# Use `sklearn.model_selection.train_test_split` method call, passing\n",
    "# arguments `test_size=args.test_size, random_state=args.seed`.\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "                                        X, y, test_size=args.test_size, random_state=args.seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "14fcb804-3c98-496c-bd52-7aa4b2444ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_integer_only(col : np.ndarray) -> bool:\n",
    "    return np.all(col == np.round(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7591b55f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Module 'sklearn' has no attribute 'StandardScaler'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/2025ws/ML/tut/lab_venv/lib/python3.13/site-packages/sklearn/__init__.py:136\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'StandardScaler'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[128]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     10\u001b[39m rest_ind = [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_size) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m int_ind]\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# - for the rest of the columns, normalize their values so that they\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#   have mean 0 and variance 1; use `sklearn.preprocessing.StandardScaler`.        \u001b[39;00m\n\u001b[32m     15\u001b[39m preprocess = sklearn.compose.ColumnTransformer(\n\u001b[32m     16\u001b[39m         [(\u001b[33m\"\u001b[39m\u001b[33mints\u001b[39m\u001b[33m\"\u001b[39m, sklearn.preprocessing.OneHotEncoder(handle_unknown=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, sparse_output=\u001b[38;5;28;01mFalse\u001b[39;00m), int_ind),\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mrests\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43msklearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStandardScaler\u001b[49m(), rest_ind)]\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m X_train_trans = preprocess.fit_transform(X_train)\n\u001b[32m     21\u001b[39m X_test_trans = preprocess.transform(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/2025ws/ML/tut/lab_venv/lib/python3.13/site-packages/sklearn/__init__.py:138\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModule \u001b[39m\u001b[33m'\u001b[39m\u001b[33msklearn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: Module 'sklearn' has no attribute 'StandardScaler'"
     ]
    }
   ],
   "source": [
    "# - if a column has only integer values, consider it a categorical column\n",
    "#   (days in a week, dog breed, ...; in general, integer values can also\n",
    "#   represent numerical non-categorical values, but we use this assumption\n",
    "#   for the sake of exercise). Encode the values with one-hot encoding\n",
    "#   using `sklearn.preprocessing.OneHotEncoder` (note that its output is by\n",
    "#   default sparse, you can use `sparse_output=False` to generate dense output;\n",
    "#   also use `handle_unknown=\"ignore\"` to ignore missing values in test set).\n",
    "d_size = X_train.shape[1]\n",
    "int_ind = [i for i in range(d_size) if is_integer_only(X_train[:, i])]\n",
    "rest_ind = [i for i in range(d_size) if i not in int_ind]\n",
    "\n",
    "# - for the rest of the columns, normalize their values so that they\n",
    "#   have mean 0 and variance 1; use `sklearn.preprocessing.StandardScaler`.        \n",
    "\n",
    "preprocess = sklearn.compose.ColumnTransformer(\n",
    "        [(\"ints\", sklearn.preprocessing.OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), int_ind),\n",
    "        (\"rests\", sklearn.preprocessing.StandardScaler(), rest_ind)]\n",
    ")\n",
    "\n",
    "X_train_trans = preprocess.fit_transform(X_train)\n",
    "X_test_trans = preprocess.transform(X_test)\n",
    "\n",
    "# In the output, first there should be all the one-hot categorical features,\n",
    "# and then the real-valued features. To process different dataset columns\n",
    "# differently, you can use `sklearn.compose.ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8973f-3061-4c0a-afa0-167d6ff172b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "60e62342",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_trans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[126]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TODO: To the current features, append polynomial features of order 2.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# If the input values are `[a, b, c, d]`, you should append\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# `[a^2, ab, ac, ad, b^2, bc, bd, c^2, cd, d^2]`. You can generate such polynomial\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# features either manually, or you can employ the provided transformer\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#   sklearn.preprocessing.PolynomialFeatures(2, include_bias=False)\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# which appends such polynomial features of order 2 to the given features.\u001b[39;00m\n\u001b[32m      8\u001b[39m poly = sklearn.preprocessing.PolynomialFeatures(\u001b[32m2\u001b[39m, include_bias=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X_train_poly = poly.fit_transform(\u001b[43mX_train_trans\u001b[49m)\n\u001b[32m     10\u001b[39m X_test_poly = poly.transform(X_test_trans)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_trans' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: To the current features, append polynomial features of order 2.\n",
    "# If the input values are `[a, b, c, d]`, you should append\n",
    "# `[a^2, ab, ac, ad, b^2, bc, bd, c^2, cd, d^2]`. You can generate such polynomial\n",
    "# features either manually, or you can employ the provided transformer\n",
    "#   sklearn.preprocessing.PolynomialFeatures(2, include_bias=False)\n",
    "# which appends such polynomial features of order 2 to the given features.\n",
    "\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_trans)\n",
    "X_test_poly = poly.transform(X_test_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea06d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: You can wrap all the feature processing steps into one transformer\n",
    "# by using `sklearn.pipeline.Pipeline`. Although not strictly needed, it is\n",
    "# usually comfortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97523f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit the feature preprocessing steps (the composed pipeline with all of\n",
    "# them; or the individual steps, if you prefer) on the training data (using `fit`).\n",
    "# Then transform the training data into `train_data` (with a `transform` call;\n",
    "# however, you can combine the two methods into a single `fit_transform` call).\n",
    "# Finally, transform testing data to `test_data`.\n",
    "train_data = ...\n",
    "test_data = ...\n",
    "\n",
    "# return train_data[:5], test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f95ad-cfb1-49bf-9017-0b0257ff9378",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_args = parser.parse_args([] if \"__file__\" not in globals() else None)\n",
    "    train_data, test_data = main(main_args)\n",
    "    for dataset in [train_data, test_data]:\n",
    "        for line in range(min(dataset.shape[0], 5)):\n",
    "            print(\" \".join(\"{:.4g}\".format(dataset[line, column]) for column in range(min(dataset.shape[1], 140))),\n",
    "                  *[\"...\"] if dataset.shape[1] > 140 else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321cfbe-72b8-4aeb-847c-bf74111d3ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f0acd-bcd9-4d8b-8d9f-17a4c2198060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe18626-c9ef-45ca-8b9a-a963f8b07af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
